
# üè¶ Credit Default Prediction - Binary Classification Project

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0%2B-orange)
![Pandas](https://img.shields.io/badge/Pandas-1.3%2B-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## üìã Table of Contents
- [Project Overview](#-project-overview)
- [Business Problem](#-business-problem)
- [Technical Challenge](#-technical-challenge)
- [Dataset Description](#-dataset-description)
- [Project Structure](#-project-structure)
- [Installation Guide](#-installation-guide)
- [Usage Instructions](#-usage-instructions)
- [Modeling Approach](#-modeling-approach)
- [Results & Evaluation](#-results--evaluation)
- [Deliverables](#-deliverables)
- [Key Findings](#-key-findings)
- [Business Impact](#-business-impact)
- [Future Improvements](#-future-improvements)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)

---

## üéØ Project Overview

This project builds a **binary classification system** to predict whether a credit card customer will default on their payment next month. It's a real-world banking application that helps financial institutions assess credit risk and make data-driven decisions.

### What This Project Does:
- ‚úÖ Predicts customer default probability
- ‚úÖ Handles real-world class imbalance
- ‚úÖ Compares multiple ML models
- ‚úÖ Optimizes for business metrics
- ‚úÖ Provides professional deliverables

---

## üíº Business Problem

### The Challenge:
Banks lose millions when customers default on payments. They need to:
1. **Identify high-risk customers** before they default
2. **Take preventive action** (reduce limits, increase monitoring)
3. **Balance risk and customer experience** (don't harass good customers)

### The Solution:
A machine learning model that predicts default probability based on customer's:
- Demographic information
- Payment history
- Bill amounts
- Previous payment amounts

---

## üî¨ Technical Challenge

### Key Challenges Addressed:

| Challenge | Solution |
|-----------|----------|
| **Class Imbalance** (78% non-default, 22% default) | SMOTE (Synthetic Minority Over-sampling) |
| **Feature Scaling** (age vs. dollars) | StandardScaler |
| **Model Selection** | Compare baseline vs. advanced |
| **Threshold Optimization** | F1-score based tuning |
| **Evaluation** | ROC-AUC + Confusion Matrix |

---

## üìä Dataset Description

### Source: UCI Credit Card Dataset

### Features (23 total):

#### 1. Demographic Information:
| Feature | Description | Values |
|---------|-------------|--------|
| `LIMIT_BAL` | Credit limit (NT dollars) | Continuous |
| `SEX` | Gender | 1=male, 2=female |
| `EDUCATION` | Education level | 1=graduate, 2=university, 3=high school, 4=others |
| `MARRIAGE` | Marital status | 1=married, 2=single, 3=others |
| `AGE` | Age in years | 21-79 |

#### 2. Payment History (Last 6 months):
| Feature | Description | Values |
|---------|-------------|--------|
| `PAY_0` to `PAY_6` | Repayment status | -2=no consumption, -1=paid fully, 0=paid minimum, 1=1 month late, 2=2 months late, ... 8=8+ months late |

#### 3. Bill Amounts (Last 6 months):
| Feature | Description |
|---------|-------------|
| `BILL_AMT1` to `BILL_AMT6` | Bill amount in NT dollars |

#### 4. Payment Amounts (Last 6 months):
| Feature | Description |
|---------|-------------|
| `PAY_AMT1` to `PAY_AMT6` | Amount paid in NT dollars |

### Target Variable:
| Value | Meaning |
|-------|---------|
| **0** | Customer WILL NOT default (pay their bill) |
| **1** | Customer WILL default (miss payment) |

---

## üìÅ Project Structure

```
project-02-credit-default/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/
‚îÇ   ‚îî‚îÄ‚îÄ UCI_Credit_Card.csv          # Original dataset
‚îÇ
‚îú‚îÄ‚îÄ üìÇ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ credit_default_prediction.ipynb  # Main notebook
‚îÇ
‚îú‚îÄ‚îÄ üìÇ src/
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                      # Helper functions
‚îÇ   ‚îú‚îÄ‚îÄ train.py                       # Training code
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py                     # Evaluation code
‚îÇ
‚îú‚îÄ‚îÄ üìÇ models/
‚îÇ   ‚îú‚îÄ‚îÄ logreg_model.pkl               # Saved Logistic Regression
‚îÇ   ‚îî‚îÄ‚îÄ gb_model.pkl                    # Saved Gradient Boosting
‚îÇ
‚îú‚îÄ‚îÄ üìÇ reports/
‚îÇ   ‚îú‚îÄ‚îÄ metrics.json                     # All evaluation metrics
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ figures/
‚îÇ       ‚îú‚îÄ‚îÄ confusion_matrix.png          # Confusion matrix plot
‚îÇ       ‚îú‚îÄ‚îÄ roc_curve.png                  # ROC curve plot
‚îÇ       ‚îú‚îÄ‚îÄ feature_importance.png          # Feature importance
‚îÇ       ‚îú‚îÄ‚îÄ threshold_tuning.png             # Threshold analysis
‚îÇ       ‚îî‚îÄ‚îÄ class_distribution.png           # Original class imbalance
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                       # Dependencies
‚îú‚îÄ‚îÄ README.md                               # This file
‚îî‚îÄ‚îÄ .gitignore                               # Git ignore file
```

---

## üîß Installation Guide

### Option 1: Kaggle Notebook (Recommended)

1. **Create Kaggle Account** at [kaggle.com](https://www.kaggle.com)
2. **Create New Notebook** and name it "Credit Default Prediction"
3. **Add Dataset**: Click "Add Input" ‚Üí Search "UCI Credit Card"
4. **Run Cells** sequentially

### Option 2: Local Setup

```bash
# 1. Clone repository
git clone https://github.com/yourusername/credit-default-prediction.git
cd credit-default-prediction

# 2. Create virtual environment
python -m venv venv

# 3. Activate environment
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate

# 4. Install dependencies
pip install -r requirements.txt

# 5. Download dataset
# Download from: https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset
# Place in 'data/' folder

# 6. Run Jupyter
jupyter notebook
```

### requirements.txt:
```txt
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
imbalanced-learn>=0.8.0
joblib>=1.1.0
jupyter>=1.0.0
```

---

## üöÄ Usage Instructions

### Step-by-Step Execution:

#### 1. **Data Loading & Exploration**
```python
# Load dataset
df = pd.read_csv('data/UCI_Credit_Card.csv')
# Check class distribution (78% non-default, 22% default)
```

#### 2. **Preprocessing**
```python
# Separate features and target
X = df.drop(['ID', 'default.payment.next.month'], axis=1)
y = df['default.payment.next.month']

# Train-test split (80-20, stratified)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
```

#### 3. **Handle Imbalance with SMOTE**
```python
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
# Now: 50% default, 50% non-default
```

#### 4. **Train Models**
```python
# Baseline: Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train_balanced, y_train_balanced)

# Better: Gradient Boosting
gb_model = GradientBoostingClassifier(n_estimators=100)
gb_model.fit(X_train_balanced, y_train_balanced)
```

#### 5. **Evaluate & Tune**
```python
# Find optimal threshold
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
f1_scores = 2 * (precision * recall) / (precision + recall)
best_threshold = thresholds[np.argmax(f1_scores[:-1])]
```

#### 6. **Generate Deliverables**
```python
# Confusion Matrix
plt.savefig('reports/figures/confusion_matrix.png')

# ROC Curve
plt.savefig('reports/figures/roc_curve.png')

# Metrics JSON
json.dump(metrics, open('reports/metrics.json', 'w'))
```

---

## ü§ñ Modeling Approach

### Model Comparison:

| Aspect | Logistic Regression | Gradient Boosting |
|--------|---------------------|-------------------|
| **Type** | Linear model | Ensemble of trees |
| **Complexity** | Simple | Complex |
| **Interpretability** | High (coefficients) | Medium (feature importance) |
| **Training Time** | Fast | Slow |
| **Performance** | Baseline | Better |
| **Overfitting Risk** | Low | Medium (controlled by params) |

### Hyperparameters:

**Logistic Regression:**
```python
LogisticRegression(
    random_state=42,
    max_iter=1000,
    class_weight='balanced'
)
```

**Gradient Boosting:**
```python
GradientBoostingClassifier(
    n_estimators=100,      # Number of trees
    learning_rate=0.1,      # Step size
    max_depth=3,            # Tree depth
    random_state=42,
    subsample=0.8           # Sample fraction per tree
)
```

### Threshold Tuning:
- Default threshold: 0.5
- Optimal threshold: Found by maximizing F1 score
- Custom threshold balances precision and recall

---

## üìä Results & Evaluation

### Performance Metrics:

| Model | F1 Score | ROC-AUC | Precision | Recall |
|-------|----------|---------|-----------|--------|
| Logistic Regression | 0.XX | 0.XX | 0.XX | 0.XX |
| Gradient Boosting | 0.XX | 0.XX | 0.XX | 0.XX |
| Gradient Boosting (Tuned) | 0.XX | 0.XX | 0.XX | 0.XX |

### Confusion Matrix (Optimal Threshold):

```
                 Predicted
              Non-Default  Default
Actual
Non-Default      X,XXX       XXX
Default           XXX       X,XXX
```

### Interpretation:
- **True Negatives**: Correctly identified good customers
- **False Positives**: Good customers flagged as risky
- **False Negatives**: Defaulters we missed (most costly)
- **True Positives**: Defaulters we caught

---

## üì¶ Deliverables

### 1. `reports/metrics.json`
```json
{
  "model_comparison": {
    "logistic_regression": {
      "f1_score": 0.XX,
      "roc_auc": 0.XX
    },
    "gradient_boosting": {
      "f1_score": 0.XX,
      "roc_auc": 0.XX
    }
  },
  "threshold_tuning": {
    "optimal_threshold": 0.XX,
    "f1_score_before": 0.XX,
    "f1_score_after": 0.XX,
    "improvement": 0.XX
  },
  "confusion_matrices": {
    "default_threshold": {...},
    "optimal_threshold": {...}
  }
}
```

### 2. `reports/figures/confusion_matrix.png`
- Visual comparison of default vs. optimal threshold
- Shows improvement in catching defaults

### 3. `reports/figures/roc_curve.png`
- ROC curves for both models
- AUC scores displayed
- Optimal threshold marked

### 4. `reports/figures/feature_importance.png`
- Top 10 most important features
- Helps explain model decisions

### 5. `reports/figures/threshold_tuning.png`
- F1 score across thresholds
- Shows why optimal threshold is chosen

---

## üí° Key Findings

### 1. **Class Imbalance Matters**
- Without SMOTE, model predicts "non-default" for everyone
- SMOTE improved default detection by XX%

### 2. **Gradient Boosting Outperforms Logistic Regression**
- XX% higher F1 score
- XX% higher ROC-AUC
- Captures non-linear relationships

### 3. **Threshold Tuning Improves Business Metrics**
- Optimal threshold: X.XX (not 0.5!)
- Caught XX more defaulters
- Only increased false alarms by XX

### 4. **Most Important Features**
1. `PAY_0` - Most recent payment status
2. `LIMIT_BAL` - Credit limit
3. `PAY_AMT1` - Most recent payment amount
4. `AGE` - Customer age
5. `BILL_AMT1` - Most recent bill amount

---

## üí∞ Business Impact

### Before Model:
- Bank misses XX% of future defaulters
- Losses: $X million annually

### With Model:
- Catch XX% of defaulters
- Reduce losses by XX%
- Save $X million annually

### Risk Segmentation:
| Risk Level | % Customers | Action |
|------------|-------------|--------|
| Low (prob < 0.3) | XX% | Approve, offer promotions |
| Medium (0.3-0.6) | XX% | Monitor closely |
| High (prob > 0.6) | XX% | Reduce limit, increase oversight |

---

## üöÄ Future Improvements

### Short-term:
- [ ] Try XGBoost instead of Gradient Boosting
- [ ] Implement cross-validation
- [ ] Add more feature engineering
- [ ] Test different SMOTE ratios

### Long-term:
- [ ] Build web app for real-time predictions
- [ ] Add explainability (SHAP values)
- [ ] Deploy as API
- [ ] A/B test in production
- [ ] Continuous learning pipeline

---

## üîç Troubleshooting

### Common Issues & Solutions:

| Issue | Solution |
|-------|----------|
| **FileNotFoundError** | Check dataset path, use `find_dataset()` function |
| **Memory Error** | Reduce batch size, use chunking |
| **SMOTE ValueError** | Ensure features are numeric and scaled |
| **Poor Performance** | Try different hyperparameters, more trees |
| **Overfitting** | Reduce max_depth, increase min_samples_split |

### Kaggle-Specific:
```python
# If dataset not found:
import os
print(os.listdir('/kaggle/input'))
# Then use correct path
```

---

## ü§ù Contributing

We welcome contributions! Here's how:

1. **Fork the repository**
2. **Create feature branch** (`git checkout -b feature/AmazingFeature`)
3. **Commit changes** (`git commit -m 'Add AmazingFeature'`)
4. **Push to branch** (`git push origin feature/AmazingFeature`)
5. **Open Pull Request**

### Contribution Ideas:
- Add more models (Random Forest, XGBoost)
- Improve feature engineering
- Add model explainability
- Create Streamlit dashboard
- Write unit tests

---

## üìÑ License

This project is licensed under the MIT License - see below:

```
MIT License

Copyright (c) 2024 School of AI - Machine Learning Internship Program

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files...
```

---

## üìû Contact

### Project Maintainer:
**School of AI - Machine Learning Internship Program**

- üìß Email: schoolofai@example.com
- üåê Website: [schoolofai.example.com](https://schoolofai.example.com)
- üê¶ Twitter: [@SchoolOfAI](https://twitter.com/SchoolOfAI)
- üíº LinkedIn: [School of AI](https://linkedin.com/company/schoolofai)

### Created by:
**Eman Aslam** (Machine Learning Intern)

- üìß Email: emanaslam543@gmail.com
- üíº LinkedIn: [emanaslamkhan](https://linkedin.com/in/emanaslamkhan)
- üêô GitHub: [@EmanAslam221522](https://github.com/EmanAslam221522)

---

## ‚≠ê Acknowledgments

- **UCI Machine Learning Repository** for the dataset
- **Scikit-learn** team for amazing ML tools
- **Kaggle** for providing computation resources
- **School of AI** for guidance and mentorship

---

## üìö References

1. [UCI Credit Card Dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)
2. [SMOTE Paper](https://arxiv.org/abs/1106.1813)
3. [Gradient Boosting Paper](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
4. [Scikit-learn Documentation](https://scikit-learn.org/)
5. [Handling Imbalanced Datasets](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)

---

## üéì School of AI - Machine Learning Internship Program

This project is part of the **School of AI Machine Learning Internship Program** - Project #2: Credit Default Prediction.



## ‚úÖ Quick Checklist

- [x] Dataset loaded and explored
- [x] Class imbalance identified
- [x] SMOTE applied to training data
- [x] Features scaled
- [x] Logistic Regression trained (baseline)
- [x] Gradient Boosting trained (better model)
- [x] F1 and ROC-AUC calculated
- [x] Threshold tuning performed
- [x] Confusion matrix generated
- [x] ROC curve generated
- [x] metrics.json saved
- [x] All deliverables ready

---

## üéâ Final Note

This project demonstrates a complete machine learning pipeline for a real-world banking problem. It handles the core challenge of class imbalance, compares multiple models, optimizes for business metrics, and produces professional deliverables.

**Remember:** In imbalanced classification, accuracy is misleading. Always use F1, ROC-AUC, and confusion matrices!

---

**‚≠ê If you find this project helpful, please star it on GitHub!** ‚≠ê

