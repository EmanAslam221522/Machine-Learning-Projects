

# ğŸ¦ Credit Default Prediction

### Binary Classification using Machine Learning

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.0%2B-orange)
![Pandas](https://img.shields.io/badge/Pandas-1.3%2B-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## ğŸ“‹ Table of Contents

* Project Overview
* Business Problem
* Technical Challenges
* Dataset Description
* Project Structure
* Installation
* Usage
* Modeling Approach
* Results & Evaluation
* Deliverables
* Key Findings
* Business Impact
* Future Work
* Troubleshooting
* Contributing
* License
* Contact

---

## ğŸ¯ Project Overview

This project implements a **binary classification system** to predict whether a credit card customer will **default on payment** in the next month.

The system helps financial institutions reduce risk by identifying high-risk customers using machine learning.

### Key Objectives

âœ” Predict default probability
âœ” Handle class imbalance
âœ” Compare baseline and advanced models
âœ” Optimize decision threshold
âœ” Generate professional reports
âœ” Provide reproducible pipeline

---

## ğŸ’¼ Business Problem

### Background

Credit default leads to significant financial losses for banks. Traditional manual assessment is slow and inconsistent.

Banks need an automated system to:

1. Identify risky customers early
2. Take preventive actions
3. Reduce financial losses
4. Improve credit management

### Solution

We build a machine learning model that analyzes:

* Customer demographics
* Payment behavior
* Billing history
* Repayment patterns

to predict default risk.

---

## ğŸ”¬ Technical Challenges

| Challenge                | Solution                   |
| ------------------------ | -------------------------- |
| Class Imbalance          | SMOTE Oversampling         |
| Feature Scale Difference | StandardScaler             |
| Model Selection          | Baseline vs Advanced       |
| Threshold Selection      | F1-based Optimization      |
| Evaluation               | ROC-AUC & Confusion Matrix |

---

## ğŸ“Š Dataset Description

### Source

UCI Credit Card Dataset

### Feature Groups

#### 1ï¸âƒ£ Demographics

| Feature   | Description     |
| --------- | --------------- |
| LIMIT_BAL | Credit limit    |
| SEX       | Gender          |
| EDUCATION | Education level |
| MARRIAGE  | Marital status  |
| AGE       | Age             |

#### 2ï¸âƒ£ Payment Status (6 Months)

PAY_0 to PAY_6 â†’ Repayment history

#### 3ï¸âƒ£ Bill Amounts

BILL_AMT1 to BILL_AMT6

#### 4ï¸âƒ£ Payment Amounts

PAY_AMT1 to PAY_AMT6

### Target Variable

| Value | Meaning    |
| ----- | ---------- |
| 0     | No Default |
| 1     | Default    |

---

## ğŸ“ Project Structure

```
credit-default-prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ UCI_Credit_Card.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ main_notebook.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ logistic_model.pkl
â”‚   â””â”€â”€ gb_model.pkl
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ metrics.json
â”‚   â””â”€â”€ figures/
â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚       â”œâ”€â”€ roc_curve.png
â”‚       â”œâ”€â”€ feature_importance.png
â”‚       â””â”€â”€ threshold_analysis.png
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ”§ Installation

### Option 1: Kaggle (Recommended)

1. Open Kaggle
2. Create Notebook
3. Add Dataset
4. Run notebook cells

### Option 2: Local Setup

```bash
git clone https://github.com/yourusername/credit-default-prediction.git
cd credit-default-prediction

python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

pip install -r requirements.txt
```

---

## ğŸš€ Usage

### 1ï¸âƒ£ Load Dataset

```python
df = pd.read_csv("data/UCI_Credit_Card.csv")
```

### 2ï¸âƒ£ Preprocessing

```python
X = df.drop(["ID", "default.payment.next.month"], axis=1)
y = df["default.payment.next.month"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y
)
```

### 3ï¸âƒ£ Handle Imbalance

```python
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
```

### 4ï¸âƒ£ Train Models

```python
log_model = LogisticRegression()
gb_model = GradientBoostingClassifier()

log_model.fit(X_train_res, y_train_res)
gb_model.fit(X_train_res, y_train_res)
```

### 5ï¸âƒ£ Evaluate

```python
y_pred = gb_model.predict(X_test)
```

---

## ğŸ¤– Modeling Approach

### Models Used

| Model               | Purpose              |
| ------------------- | -------------------- |
| Logistic Regression | Baseline             |
| Gradient Boosting   | Improved Performance |

### Hyperparameters

**Logistic Regression**

```python
LogisticRegression(max_iter=1000, class_weight="balanced")
```

**Gradient Boosting**

```python
GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
```

### Threshold Optimization

Instead of fixed 0.5, we optimize threshold using F1-score.

---

## ğŸ“Š Results & Evaluation

| Model               | F1 | ROC-AUC | Precision | Recall |
| ------------------- | -- | ------- | --------- | ------ |
| Logistic Regression | XX | XX      | XX        | XX     |
| Gradient Boosting   | XX | XX      | XX        | XX     |

### Confusion Matrix

```
Actual / Predicted
------------------
TN | FP
FN | TP
```

### Interpretation

* FN = Most expensive mistake
* TP = Correctly caught defaulters

---

## ğŸ“¦ Deliverables

### âœ” metrics.json

```json
{
  "logistic": {
    "f1": 0.XX,
    "roc_auc": 0.XX
  },
  "gradient_boosting": {
    "f1": 0.XX,
    "roc_auc": 0.XX
  }
}
```

### âœ” Plots

* confusion_matrix.png
* roc_curve.png
* feature_importance.png
* threshold_analysis.png

---

## ğŸ’¡ Key Findings

1. SMOTE significantly improved recall
2. Gradient Boosting outperformed Logistic Regression
3. Threshold tuning reduced false negatives
4. Payment history is most influential feature

---

## ğŸ’° Business Impact

| Metric            | Before ML | After ML  |
| ----------------- | --------- | --------- |
| Default Detection | Low       | High      |
| Loss              | High      | Reduced   |
| Monitoring Cost   | High      | Optimized |

### Risk Groups

| Risk   | Action   |
| ------ | -------- |
| Low    | Approve  |
| Medium | Monitor  |
| High   | Restrict |

---

## ğŸš€ Future Work

### Short Term

* Cross-validation
* XGBoost
* Feature engineering

### Long Term

* Web application
* API deployment
* SHAP explainability
* Real-time scoring
* MLOps pipeline

---

## ğŸ” Troubleshooting

| Problem           | Fix            |
| ----------------- | -------------- |
| Dataset not found | Check path     |
| Low accuracy      | Tune params    |
| Overfitting       | Reduce depth   |
| Memory error      | Batch training |

Kaggle path check:

```python
import os
os.listdir("/kaggle/input")
```

---

## ğŸ¤ Contributing

1. Fork repo
2. Create branch
3. Commit changes
4. Push
5. Open PR

Contribution Ideas:

* Add new models
* Improve visualization
* Build dashboard
* Add tests

---

## ğŸ“„ License

MIT License

Free for educational and commercial use.

---

## ğŸ“ Contact

### Author

**Eman Aslam**
Machine Learning Engineer

ğŸ“§ [emanaslam543@gmail.com](mailto:emanaslam543@gmail.com)
ğŸ’¼ LinkedIn: emanaslamkhan
ğŸ™ GitHub: EmanAslam221522

---

## â­ Acknowledgments

* UCI Repository
* Scikit-learn Team
* Kaggle
* Open Source Community

---

## ğŸ“ Internship Project

This project was developed as part of a Machine Learning Internship Program.


---

## ğŸ‰ Final Note

This project demonstrates an end-to-end machine learning workflow for financial risk prediction.

It focuses on **data quality, model evaluation, and business relevance**, which are essential in real-world ML systems.

â­ If you find this useful, please star the repository!

